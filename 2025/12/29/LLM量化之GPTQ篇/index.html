
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8" />
    <title>LLM量化之GPTQ篇 | Alielie&#39;s Blog</title>
    <meta name="author" content="Alielie" />
    <meta name="description" content="" />
    <meta name="keywords" content="" />
    <meta
        name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0"
    />
    <link rel="icon" href="/images/head.png" />
    <link rel="preconnect" href="https://s4.zstatic.net" />
<script src="https://s4.zstatic.net/ajax/libs/vue/3.3.7/vue.global.prod.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" />
<link rel="preconnect" href="https://fonts.googleapis.cn" />
<link rel="preconnect" href="https://fonts.gstatic.cn" crossorigin />
<link
    rel="stylesheet"
    href="https://fonts.googleapis.cn/css2?family=Fira+Code:wght@400;500;600;700&family=Lexend:wght@400;500;600;700;800;900&family=Noto+Sans+SC:wght@400;500;600;700;800;900&display=swap"
/>
<script> const mixins = {}; </script>

<script src="https://polyfill.alicdn.com/v3/polyfill.min.js?features=default"></script>


<script src="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/highlightjs-line-numbers.js/2.8.0/highlightjs-line-numbers.min.js"></script>
<link
    rel="stylesheet"
    href="https://s4.zstatic.net/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
/>
<script src="/js/lib/highlight.js"></script>


<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.js"></script>
<script src="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/contrib/auto-render.min.js"></script>
<link rel="stylesheet" href="https://s4.zstatic.net/ajax/libs/KaTeX/0.16.9/katex.min.css" />
<script src="/js/lib/math.js"></script>


<script src="/js/lib/preview.js"></script>









<link rel="stylesheet" href="/css/main.css" />
<link rel="stylesheet" href="/css/custom.css" />
<meta name="generator" content="Hexo 8.1.1"></head>
<body>
    <div id="layout">
        <transition name="fade">
            <div id="loading" v-show="loading">
                <div id="loading-circle">
                    <h2>LOADING</h2>
                    <p>加载过慢请开启缓存 浏览器默认开启</p>
                    <img src="/images/loading.gif" />
                </div>
            </div>
        </transition>
        <div id="menu" :class="{ hidden: hiddenMenu, 'menu-color': menuColor}">
    <nav id="desktop-menu">
        <a class="title" href="/">
            <span>AWEN</span>
        </a>
        
        <a href="/">
            <i class="fa-solid fa-house fa-fw"></i>
            <span>&ensp;Home</span>
        </a>
        
        <a href="/about">
            <i class="fa-solid fa-id-card fa-fw"></i>
            <span>&ensp;About</span>
        </a>
        
        <a href="/archives">
            <i class="fa-solid fa-box-archive fa-fw"></i>
            <span>&ensp;Archives</span>
        </a>
        
        <a href="/categories">
            <i class="fa-solid fa-bookmark fa-fw"></i>
            <span>&ensp;Categories</span>
        </a>
        
        <a href="/tags">
            <i class="fa-solid fa-tags fa-fw"></i>
            <span>&ensp;Tags</span>
        </a>
        
    </nav>
    <nav id="mobile-menu">
        <div class="title" @click="showMenuItems = !showMenuItems">
            <i class="fa-solid fa-bars fa-fw"></i>
            <span>&emsp;AWEN</span>
        </div>
        <transition name="slide">
            <div class="items" v-show="showMenuItems">
                
                <a href="/">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-house fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Home</div>
                    </div>
                </a>
                
                <a href="/about">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-id-card fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">About</div>
                    </div>
                </a>
                
                <a href="/archives">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-box-archive fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Archives</div>
                    </div>
                </a>
                
                <a href="/categories">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-bookmark fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Categories</div>
                    </div>
                </a>
                
                <a href="/tags">
                    <div class="item">
                        <div style="min-width: 20px; max-width: 50px; width: 10%">
                            <i class="fa-solid fa-tags fa-fw"></i>
                        </div>
                        <div style="min-width: 100px; max-width: 150%; width: 20%">Tags</div>
                    </div>
                </a>
                
            </div>
        </transition>
    </nav>
</div>
<transition name="fade">
    <div id="menu-curtain" @click="showMenuItems = !showMenuItems" v-show="showMenuItems"></div>
</transition>
        <div id="main" :class="loading ? 'into-enter-from': 'into-enter-active'">
            <div class="article">
    <div>
        <h1>LLM量化之GPTQ篇</h1>
    </div>
    <div class="info">
        <span class="date">
            <span class="icon">
                <i class="fa-solid fa-calendar fa-fw"></i>
            </span>
            2025/12/29
        </span>
        
        
    </div>
    
    <div class="content" v-pre>
        <h1 id="大模型量化与混合精度详解"><a href="#大模型量化与混合精度详解" class="headerlink" title="大模型量化与混合精度详解"></a>大模型量化与混合精度详解</h1><h2 id="1-1-核心概念辨析"><a href="#1-1-核心概念辨析" class="headerlink" title="1.1 核心概念辨析"></a>1.1 核心概念辨析</h2><p>为了厘清容易混淆的概念，我们需要区分”混合精度训练”与”大模型量化”的应用场景与目的。</p>
<table>
<thead>
<tr>
<th align="left">概念</th>
<th align="left">场景</th>
<th align="left">数据格式</th>
<th align="left">核心目的</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>混合精度训练 (AMP)</strong></td>
<td align="left">训练&#x2F;微调</td>
<td align="left">FP16 + FP32</td>
<td align="left">在保持精度的前提下，减少显存占用，加速<strong>训练</strong>过程。</td>
</tr>
<tr>
<td align="left"><strong>大模型量化 (Quantization)</strong></td>
<td align="left">推理&#x2F;部署</td>
<td align="left">INT8 &#x2F; INT4</td>
<td align="left">降低参数精度，大幅减少显存占用，提高<strong>推理</strong>速度，使其能在消费级硬件上运行。</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>注</strong>：量化通常指的是将高精度浮点数（FP16&#x2F;FP32）映射为低精度整数（INT8&#x2F;INT4）。</p>
</blockquote>
<hr>
<h2 id="1-2-量化技术流派"><a href="#1-2-量化技术流派" class="headerlink" title="1.2 量化技术流派"></a>1.2 量化技术流派</h2><p>量化技术主要分为两大流派，目前普通用户和开发者接触最多的是 PTQ。</p>
<h3 id="1-PTQ-Post-Training-Quantization-训练后量化"><a href="#1-PTQ-Post-Training-Quantization-训练后量化" class="headerlink" title="1. PTQ (Post-Training Quantization, 训练后量化)"></a>1. PTQ (Post-Training Quantization, 训练后量化)</h3><ul>
<li><strong>定义</strong>：模型训练完成后，直接对权重&#x2F;激活值进行压缩。</li>
<li><strong>特点</strong>：无需重新训练，计算资源消耗低。</li>
<li><strong>代表技术</strong>：<strong>GPTQ</strong>, <strong>AWQ</strong>, <strong>GGUF</strong>, <strong>LLM.int8()</strong>。</li>
</ul>
<h3 id="2-QAT-Quantization-Aware-Training-量化感知训练"><a href="#2-QAT-Quantization-Aware-Training-量化感知训练" class="headerlink" title="2. QAT (Quantization-Aware Training, 量化感知训练)"></a>2. QAT (Quantization-Aware Training, 量化感知训练)</h3><ul>
<li><strong>定义</strong>：在训练过程中模拟量化操作，让模型在训练时就”适应”低精度带来的误差。</li>
<li><strong>特点</strong>：精度损失最小，但训练难度大、成本极高。</li>
</ul>
<hr>
<h2 id="1-3-量化的数学原理"><a href="#1-3-量化的数学原理" class="headerlink" title="1.3 量化的数学原理"></a>1.3 量化的数学原理</h2><p>量化的本质是建立浮点数（$r$）与整数（$q$）之间的映射关系。</p>
<h3 id="1-对称量化-Symmetric-Quantization"><a href="#1-对称量化-Symmetric-Quantization" class="headerlink" title="1. 对称量化 (Symmetric Quantization)"></a>1. 对称量化 (Symmetric Quantization)</h3><p>通常用于 <strong>AbsMax 量化</strong>。这种方法计算简单，但对非对称分布的数据（如 ReLU 激活后的数据）可能造成范围浪费。</p>
<ul>
<li><strong>步骤 1：寻找绝对值最大值</strong><br>$$ \text{absmax} &#x3D; \max(|X|) $$</li>
<li><strong>步骤 2：计算缩放因子 (Scale)</strong><br>由于 INT8 的表示范围为 $[-127, 127]$：<br>$$ S &#x3D; \frac{\text{absmax}}{127} $$</li>
<li><strong>步骤 3：量化映射</strong><br>$$ X_{\text{int8}} &#x3D; \text{round}\left( \text{clamp}\left( \frac{X_{\text{fp32}}}{S}, -127, 127 \right) \right) $$</li>
<li><strong>步骤 4：反量化 (Dequantization)</strong><br>$$ X_{\text{fp32}} \approx X_{\text{int8}} \times S $$</li>
</ul>
<h3 id="2-非对称量化-Asymmetric-Zeropoint-Quantization"><a href="#2-非对称量化-Asymmetric-Zeropoint-Quantization" class="headerlink" title="2. 非对称量化 (Asymmetric &#x2F; Zeropoint Quantization)"></a>2. 非对称量化 (Asymmetric &#x2F; Zeropoint Quantization)</h3><p>通过引入零点（Zero-point），解决数据分布不对称的问题（即数据的 0 点并不对应整数的 0 点）。</p>
<p>假设浮点数范围为 $[r_{\min}, r_{\max}]$，目标整数范围为 $[q_{\min}, q_{\max}]$。映射公式为线性变换：<br>$$ q &#x3D; \text{round}\left( \frac{r}{S} + Z \right) $$</p>
<ul>
<li><strong>(1) 缩放系数 (Scale):</strong><br>$$ S &#x3D; \frac{r_{\max} - r_{\min}}{q_{\max} - q_{\min}} $$</li>
<li><strong>(2) 零点 (Zero-point):</strong><br>$$ Z &#x3D; \text{round}\left( q_{\min} - \frac{r_{\min}}{S} \right) $$</li>
<li><strong>(3) 反量化过程:</strong><br>$$ r \approx S \times (q - Z) $$</li>
</ul>
<h3 id="3-百分位数量化（都是基于上述原理）"><a href="#3-百分位数量化（都是基于上述原理）" class="headerlink" title="3. 百分位数量化（都是基于上述原理）"></a>3. 百分位数量化（都是基于上述原理）</h3><p>为了减轻上述两种算法对离群值的敏感性，在校准期间收集数值的分布，然后确定上下限的百分数阈值，比如 0.1% 和 99.9%，那么在这个范围外的数都会变为最大值或最小值。</p>
<p><strong>优点：</strong></p>
<ul>
<li>比 MinMax 对异常值更具抵抗力。</li>
<li>实现相对简单，只需收集直方图和计算百分位数。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>对被截断的异常值引入饱和误差。如果这些异常值带有重要信息，截断它们可能会对精度产生负面影响。</li>
<li>需要选择合适的百分位数，这成为一个需要调整的超参数。最佳百分位数可能因不同层或模型而异。</li>
</ul>
<h3 id="4-熵（KL散度）量化"><a href="#4-熵（KL散度）量化" class="headerlink" title="4. 熵（KL散度）量化"></a>4. 熵（KL散度）量化</h3><p>采用更科学的方式选择这个截断范围，**KL 散度：**平衡两个分布（$P$ 和 $Q$）之间差异度。希望让截断后量化的数据分布和原始的数据分布最像。</p>
<p><strong>具体步骤：</strong></p>
<ol>
<li><strong>准备</strong>：统计原始数据分布 $P$（FP32）。</li>
<li><strong>遍历测试</strong>：假设我们尝试很多个不同的截断阈值（比如 $T&#x3D;1, T&#x3D;2, …, T&#x3D;100$）。</li>
<li><strong>模拟量化</strong>：对于每一个 $T$：<ul>
<li>把数据截断到 $[-T, T]$ 并量化。</li>
<li>然后再反量化回来，得到分布 $Q$。</li>
</ul>
</li>
<li><strong>对比</strong>：计算 $P$ 和 $Q$ 之间的 KL 散度。</li>
<li><strong>决策</strong>：找到那个<strong>KL 散度最小</strong>的 $T$，这就是最佳截断阈值。</li>
</ol>
<ul>
<li><strong>优点</strong>：<ul>
<li><strong>抗干扰</strong></li>
<li><strong>精度高</strong>：因为范围变小了（比如从 $[-1, 1000]$ 变成了 $[-5, 5]$），每一格（Step）代表的数值更精细了，主要数据被照顾得更好。</li>
</ul>
</li>
<li><strong>缺点</strong>：<ul>
<li><strong>信息丢失</strong>：被切掉的那 $0.1%$ 数据如果包含重要信息（比如某些特殊的激活信号），模型可能会变傻。</li>
<li><strong>调参麻烦</strong>：选 $99.9%$ 还是 $99%$？不同模型、不同层可能不一样，需要试。</li>
</ul>
</li>
</ul>
<h2 id="2-算法选择指南"><a href="#2-算法选择指南" class="headerlink" title="2. 算法选择指南"></a>2. 算法选择指南</h2><p>根据数据的<strong>分布特征</strong>来决定：</p>
<table>
<thead>
<tr>
<th align="left">目标对象</th>
<th align="left">数据特征</th>
<th align="left">推荐算法</th>
<th align="left">原因</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>权重 (Weights)</strong></td>
<td align="left"><strong>稳定、对称</strong>(通常像正态分布)</td>
<td align="left"><strong>MinMax</strong> 或 <strong>百分位数</strong></td>
<td align="left">权重是训练好的，通常没有极端离群值，且分布比较”漂亮”，简单方法就够用。</td>
</tr>
<tr>
<td align="left"><strong>激活值 (Activations)</strong></td>
<td align="left"><strong>不稳定、非对称、长尾</strong>(可能有巨大离群值)</td>
<td align="left"><strong>熵量化 (KL)</strong> 或 <strong>百分位数</strong></td>
<td align="left">激活值随输入变化，且经常有怪异的极值（如 ReLU 后的长尾）。MinMax 会导致精度崩塌，必须用截断策略。</td>
</tr>
</tbody></table>
<h3 id="总结对比表"><a href="#总结对比表" class="headerlink" title="总结对比表"></a>总结对比表</h3><table>
<thead>
<tr>
<th align="left">算法</th>
<th align="left">核心逻辑</th>
<th align="left">速度</th>
<th align="left">对离群值抵抗力</th>
<th align="left">精度上限</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>MinMax</strong></td>
<td align="left">以此为界，全都要</td>
<td align="left">最快</td>
<td align="left">极差 (被带跑偏)</td>
<td align="left">低 (易受干扰)</td>
</tr>
<tr>
<td align="left"><strong>百分位数</strong></td>
<td align="left">切掉头尾，保留中间</td>
<td align="left">中等</td>
<td align="left">强 (强制切除)</td>
<td align="left">中高 (依赖调参)</td>
</tr>
<tr>
<td align="left"><strong>熵 (KL)</strong></td>
<td align="left">算出来的”最像”截断点</td>
<td align="left">最慢</td>
<td align="left">强 (智能平衡)</td>
<td align="left"><strong>最高</strong></td>
</tr>
</tbody></table>
<hr>
<h2 id="3-LLM-int8-技术详解"><a href="#3-LLM-int8-技术详解" class="headerlink" title="3. LLM.int8() 技术详解"></a>3. LLM.int8() 技术详解</h2><p><strong>LLM.int8()</strong> 本质上属于 <strong>PTQ（在线动态量化）</strong>。它的核心贡献在于解决了大模型中”离群值”导致量化精度崩塌的问题。</p>
<h3 id="核心思想：混合精度分解-Mixed-Precision-Decomposition"><a href="#核心思想：混合精度分解-Mixed-Precision-Decomposition" class="headerlink" title="核心思想：混合精度分解 (Mixed Precision Decomposition)"></a>核心思想：混合精度分解 (Mixed Precision Decomposition)</h3><p>大模型中绝大部分数值分布集中，适合量化；但极少数特征维度存在数值巨大的<strong>系统性离群值 (Outliers)</strong>。如果强行统一量化，这些离群值会拉大 Range，导致正常值的量化分辨率极低。</p>
<p><strong>解决方案</strong>：根据阈值 $\alpha$ 将矩阵分解为两部分，分别计算。</p>
<ol>
<li><p><strong>分解矩阵</strong>：<br>$$ X &#x3D; X_{\text{regular}} + X_{\text{outlier}} $$<br>$$ W &#x3D; W_{\text{regular}} + W_{\text{outlier}} $$</p>
<blockquote>
<p>注：这里 $X_{\text{outlier}}$ 仅保留超过阈值的列，其余位置补 0；$X_{\text{regular}}$ 则保留其余部分。</p>
</blockquote>
</li>
<li><p><strong>分流计算</strong>：</p>
<ul>
<li><strong>离群部分 ($X_{\text{outlier}}$)</strong>：保持 <strong>FP16</strong> 高精度计算，确保不损失精度。</li>
<li><strong>正常部分 ($X_{\text{regular}}$)</strong>：转换为 <strong>INT8</strong> 进行向量化计算，提高效率。</li>
</ul>
</li>
<li><p><strong>合并结果</strong>：<br>由于分解是基于维度的（类似于掩码操作），交叉项乘积为 0，因此：<br>$$ \text{Result} \approx (X_{\text{regular}} \times W_{\text{regular}})<em>{\text{INT8}} + (X</em>{\text{outlier}} \times W_{\text{outlier}})_{\text{FP16}} $$</p>
</li>
</ol>
<h2 id="4-补充：LLM-int8-的具体动态量化与推理流程"><a href="#4-补充：LLM-int8-的具体动态量化与推理流程" class="headerlink" title="4. 补充：LLM.int8() 的具体动态量化与推理流程"></a>4. 补充：LLM.int8() 的具体动态量化与推理流程</h2><p>LLM.int8() 的核心魔法在于**推理时（Inference-time）**的动态处理。它不仅仅是将模型存为 8-bit，更关键的是在计算矩阵乘法（$Y &#x3D; X \times W$）时，如何实时处理数据。</p>
<h3 id="1-前置概念：逐向量量化-Vector-wise-Quantization"><a href="#1-前置概念：逐向量量化-Vector-wise-Quantization" class="headerlink" title="1. 前置概念：逐向量量化 (Vector-wise Quantization)"></a>1. 前置概念：逐向量量化 (Vector-wise Quantization)</h3><p>传统的量化可能整个矩阵共用一个缩放因子 $S$（Per-tensor），这会导致精度损失。LLM.int8() 采用更精细的策略：</p>
<ul>
<li><strong>对于权重 $W$</strong>：每一<strong>列</strong>都有独立的缩放常数。</li>
<li><strong>对于输入（激活值）$X$</strong>：每一<strong>行</strong>（即每个 Token）都有独立的缩放常数。<br>这保证了量化的精度基础。</li>
</ul>
<h3 id="2-详细推理流程-Step-by-Step"><a href="#2-详细推理流程-Step-by-Step" class="headerlink" title="2. 详细推理流程 (Step-by-Step)"></a>2. 详细推理流程 (Step-by-Step)</h3><p>假设我们正在进行一层 Transformer 的计算，输入是隐藏层状态 $X$（FP16），权重是 $W$（已存储为 INT8）。</p>
<h4 id="第一步：离群值检测-Outlier-Detection"><a href="#第一步：离群值检测-Outlier-Detection" class="headerlink" title="第一步：离群值检测 (Outlier Detection)"></a>第一步：离群值检测 (Outlier Detection)</h4><p>当输入数据 $X$（FP16）到来时，系统会扫描 $X$ 的每一个元素。</p>
<ul>
<li><strong>规则</strong>：检查每一列（特征维度）中是否存在绝对值超过阈值（通常为 <strong>6.0</strong>）的元素。</li>
<li><strong>结果</strong>：找到那些包含”剧烈波动”数值的特征列索引，记为 $I_{\text{outlier}}$。<blockquote>
<p><strong>为什么叫动态？</strong> 因为 $X$ 是用户输入产生的，每一轮对话的 $X$ 都不一样，所以离群值的列也是实时变化的。</p>
</blockquote>
</li>
</ul>
<h4 id="第二步：混合精度分解-Decomposition"><a href="#第二步：混合精度分解-Decomposition" class="headerlink" title="第二步：混合精度分解 (Decomposition)"></a>第二步：混合精度分解 (Decomposition)</h4><p>根据上一步找到的索引 $I_{\text{outlier}}$，将矩阵 $X$ 和权重矩阵 $W$ 实时”撕”成两半：</p>
<ol>
<li><p><strong>离群部分 (FP16 通道)</strong>：</p>
<ul>
<li>从 $X$ 中提取出这些列，形成 $X_{\text{fp16}}$。</li>
<li>从 $W$ 中提取出对应的行（反量化为 FP16），形成 $W_{\text{fp16}}$。</li>
<li><em>注：这部分数据量极小（通常 &lt; 0.1%），但对精度至关重要。</em></li>
</ul>
</li>
<li><p><strong>正常部分 (INT8 通道)</strong>：</p>
<ul>
<li>$X$ 中剩下的列是平稳的，将其进行 <strong>动态量化</strong>：<ol>
<li>计算每一行的绝对最大值。</li>
<li>计算缩放因子 $S_x$。</li>
<li>映射为 INT8，形成 $X_{\text{int8}}$。</li>
</ol>
</li>
<li>$W$ 中剩下的行本身就是 INT8 存储的，直接取出作为 $W_{\text{int8}}$。</li>
</ul>
</li>
</ol>
<h4 id="第三步：分流计算-Matrix-Multiplication"><a href="#第三步：分流计算-Matrix-Multiplication" class="headerlink" title="第三步：分流计算 (Matrix Multiplication)"></a>第三步：分流计算 (Matrix Multiplication)</h4><p>现在我们并行执行两次矩阵乘法：</p>
<ol>
<li><p><strong>高精度流</strong>：<br>$$ O_{\text{outlier}} &#x3D; X_{\text{fp16}} \times W_{\text{fp16}} $$<br><em>(结果是 FP16)</em></p>
</li>
<li><p><strong>低精度流</strong>：<br>$$ O_{\text{regular}} &#x3D; X_{\text{int8}} \times W_{\text{int8}} $$<br><em>(结果是 INT32)</em></p>
<ul>
<li><strong>反量化</strong>：计算完成后，立刻利用权重和输入的缩放因子 ($S_w, S_x$) 将结果转回 FP16。<br>$$ O_{\text{regular_fp16}} &#x3D; O_{\text{regular}} \times S_w \times S_x $$</li>
</ul>
</li>
</ol>
<h4 id="第四步：结果合并-Merge"><a href="#第四步：结果合并-Merge" class="headerlink" title="第四步：结果合并 (Merge)"></a>第四步：结果合并 (Merge)</h4><p>将两部分结果相加，得到最终输出：<br>$$ Y_{\text{output}} &#x3D; O_{\text{outlier}} + O_{\text{regular_fp16}} $$</p>
<h3 id="3-总结：为什么它慢但效果好？"><a href="#3-总结：为什么它慢但效果好？" class="headerlink" title="3. 总结：为什么它慢但效果好？"></a>3. 总结：为什么它慢但效果好？</h3><ul>
<li><strong>效果好</strong>：因为它不像普通量化那样强制把 100 和 0.1 压在同一个 8-bit 空间里。它”尊重”了那 0.1% 的捣乱分子（离群值），给它们开了 VIP 通道（FP16）。</li>
<li><strong>速度慢</strong>：因为每次推理都要<strong>实时</strong>扫描 $X$、<strong>实时</strong>拆分矩阵、<strong>实时</strong>反量化部分权重。这比直接把整个矩阵扔进 GPU 算要繁琐得多。</li>
</ul>
<hr>
<h2 id="5-训练后量化-PTQ-流程与分类"><a href="#5-训练后量化-PTQ-流程与分类" class="headerlink" title="5. 训练后量化 (PTQ) 流程与分类"></a>5. 训练后量化 (PTQ) 流程与分类</h2><h3 id="1-PTQ-的对象分类"><a href="#1-PTQ-的对象分类" class="headerlink" title="1. PTQ 的对象分类"></a>1. PTQ 的对象分类</h3><ul>
<li><strong>Weight-Only (仅权重量化)</strong><ul>
<li><strong>含义</strong>：只压缩模型参数，激活值计算时仍用 FP16。</li>
<li><strong>优缺</strong>：精度损失极小，显存节省明显。</li>
<li><strong>代表</strong>：<strong>GPTQ</strong>, <strong>AWQ</strong>, <strong>GGUF (k-quants)</strong>。<em>(目前最主流)</em></li>
</ul>
</li>
<li><strong>Weight + Activation (全量化)</strong><ul>
<li><strong>含义</strong>：参数和激活值都量化。</li>
<li><strong>优缺</strong>：利用 INT8 Tensor Core 速度最快，但精度极易受损，需配合 SmoothQuant 等技术处理离群值。</li>
</ul>
</li>
</ul>
<h3 id="2-标准-PTQ-实施流程"><a href="#2-标准-PTQ-实施流程" class="headerlink" title="2. 标准 PTQ 实施流程"></a>2. 标准 PTQ 实施流程</h3><p>PTQ 的过程实际上就是寻找最佳 $S$ 和 $Z$ 参数并应用的过程。</p>
<ol>
<li><strong>准备模型</strong>：加载预训练的全精度模型（如 FP16&#x2F;FP32）。</li>
<li><strong>数据校准 (Calibration)</strong>：<ul>
<li>准备一个小型的、有代表性的数据集（校准集）。</li>
<li>让模型在该数据集上运行推理。</li>
</ul>
</li>
<li><strong>统计范围 (Observer)</strong>：<ul>
<li>收集每一层的权重和<strong>激活值</strong>的统计数据（主要是 $r_{\min}$ 和 $r_{\max}$）。</li>
</ul>
</li>
<li><strong>计算参数</strong>（静态体现）：<ul>
<li>根据统计数据，计算最佳的 <strong>Scale ($S$)</strong> 和 <strong>Zero-point ($Z$)</strong>。（这样在推理过程中就不需要生成而是使用先验的。</li>
<li><em>注：GPTQ&#x2F;AWQ 等算法在此步骤有更复杂的优化策略（如最小化重构误差）。</em></li>
</ul>
</li>
<li><strong>实施量化</strong>：<ul>
<li>利用 $S$ 和 $Z$ 将 FP32 权重转换为 INT8&#x2F;INT4。</li>
</ul>
</li>
<li><strong>保存模型</strong>：<ul>
<li>存储量化后的权重以及对应的量化参数（$S, Z$），供推理引擎在运行时进行反量化或直接计算。</li>
</ul>
</li>
</ol>
<h3 id="3-静态量化与动态量化"><a href="#3-静态量化与动态量化" class="headerlink" title="3. 静态量化与动态量化"></a>3. 静态量化与动态量化</h3><h4 id="静态量化"><a href="#静态量化" class="headerlink" title="静态量化"></a>静态量化</h4><p>静态量化是在执行推理<em>之前</em>对模型权重和激活进行量化。由于激活的取值范围可能因输入数据而异，静态量化需要一个校准步骤。</p>
<p><strong>优点：</strong></p>
<ul>
<li>更高的潜在性能：由于所有量化参数都预先固定，推理可以在支持的硬件（CPU、GPU、加速器）上完全使用高度优化的整数算术指令运行，从而实现最大程度的加速和能效。</li>
<li>更低的推理开销：计算激活范围或量化参数无需运行时成本。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>需要校准数据集。</li>
<li>对离群值敏感。</li>
</ul>
<p><strong>注意这里 LLM 主流的 Weight-Only 静态量化（W4A16&#x2F;W8A16）和一般的不一样：</strong> 对输入不进行量化，对权重进行反量化。</p>
<h4 id="动态量化"><a href="#动态量化" class="headerlink" title="动态量化"></a>动态量化</h4><p>动态量化，在某些情况下也常被称为”仅权重量化”，采取了不同的方法。权重在离线阶段量化，这一点与静态量化类似。然而，激活是在推理过程中<em>动态</em>或<em>即时</em>量化的。</p>
<ol>
<li><strong>离线权重量化：</strong> 模型权重被转换为低精度整数格式（例如，INT8）并存储。</li>
<li><strong>推理：</strong><ul>
<li>当输入数据到来时，激活会逐层处理。</li>
<li><strong>对于涉及量化权重的操作（如矩阵乘法），传入的浮点激活会在操作前立即量化</strong>。它们的范围（最小值&#x2F;最大值）是根据当前批次数据动态计算的。</li>
<li>计算（例如，<code>INT8</code> 权重 * <code>INT8</code> 激活）得以执行。</li>
<li>结果通常在传递给需要浮点输入的下一个操作或层（如某些激活函数或归一化层）之前，反量化回浮点数（FP32）。</li>
</ul>
</li>
</ol>
<hr>
<h1 id="量化推理中的反量化与噪声分析"><a href="#量化推理中的反量化与噪声分析" class="headerlink" title="量化推理中的反量化与噪声分析"></a>量化推理中的反量化与噪声分析</h1><h2 id="1-核心结论"><a href="#1-核心结论" class="headerlink" title="1. 核心结论"></a>1. 核心结论</h2><ul>
<li><strong>误区澄清</strong>：反量化（Dequantization）本身<strong>不产生噪声</strong>。</li>
<li><strong>真实情况</strong>：噪声（误差）是在**量化（FP16 $\to$ INT）**那一刻产生的（有损压缩）。反量化只是将”低精度整数”还原为”高精度浮点数”的格式转换（$r &#x3D; S \times q + Z$），它只是”显露”了误差，而非”增加”误差。</li>
</ul>
<h2 id="2-现代推理流程-以-W4A16-为例"><a href="#2-现代推理流程-以-W4A16-为例" class="headerlink" title="2. 现代推理流程 (以 W4A16 为例)"></a>2. 现代推理流程 (以 W4A16 为例)</h2><p>现代 LLM 推理采用**”低精度存储，高精度计算”**的策略，流程如下：</p>
<ol>
<li><strong>加载 (Load)</strong>：从显存读取 INT4 权重（<strong>节省显存</strong>）。</li>
<li><strong>反量化 (Dequant)</strong>：在计算单元（Tensor Core）附近，瞬间将 INT4 恢复为 FP16（<strong>恢复格式</strong>）。</li>
<li><strong>计算 (Compute)</strong>：使用 FP16 进行矩阵乘法（<strong>高精度环境</strong>）。<ul>
<li>$$Y_{\text{fp16}} &#x3D; X_{\text{fp16}} \times \text{Dequant}(W_{\text{int4}})$$<blockquote>
<p><strong>性能真相</strong>：W4A16 的加速主要来自<strong>显存带宽的节省</strong>（读权重快了 4 倍），而非计算加速（因为计算核心还是在跑 FP16）。</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>关键点</strong>：中间结果（Activations）始终保持 FP16 流动，避免了”每一层都重新量化”带来的二次噪声引入。</p>
</blockquote>
<h2 id="3-为什么误差不会层层累积导致崩溃？"><a href="#3-为什么误差不会层层累积导致崩溃？" class="headerlink" title="3. 为什么误差不会层层累积导致崩溃？"></a>3. 为什么误差不会层层累积导致崩溃？</h2><p>虽然初始量化有误差，但通过以下机制抑制了传播：</p>
<h3 id="A-算法层面的补偿-Calibration"><a href="#A-算法层面的补偿-Calibration" class="headerlink" title="A. 算法层面的补偿 (Calibration)"></a>A. 算法层面的补偿 (Calibration)</h3><ul>
<li><strong>技术</strong>：GPTQ, AWQ。</li>
<li><strong>原理</strong>：在量化前，通过数学方法调整未量化的参数，使得”量化后的输出”在数学期望上无限接近”原版 FP16 的输出”。误差被算法”预消化”了。</li>
</ul>
<h3 id="B-物理层面的隔离-Outlier-Protection"><a href="#B-物理层面的隔离-Outlier-Protection" class="headerlink" title="B. 物理层面的隔离 (Outlier Protection)"></a>B. 物理层面的隔离 (Outlier Protection)</h3><ul>
<li><strong>技术</strong>：LLM.int8(), AWQ。</li>
<li><strong>原理</strong>：对于极少数容易放大噪声的<strong>离群值</strong>（Outliers），<strong>不进行量化</strong>，直接使用 FP16 传输和计算。</li>
</ul>
<h3 id="C-计算层面的缓冲-High-Precision-Compute"><a href="#C-计算层面的缓冲-High-Precision-Compute" class="headerlink" title="C. 计算层面的缓冲 (High Precision Compute)"></a>C. 计算层面的缓冲 (High Precision Compute)</h3><ul>
<li><strong>原理</strong>：利用 FP16（甚至 FP32 累加器）的高精度计算环境，来消化低精度权重带来的微小扰动。</li>
</ul>
<hr>
<h1 id="GPTQ-算法详解"><a href="#GPTQ-算法详解" class="headerlink" title="GPTQ 算法详解"></a>GPTQ 算法详解</h1><h2 id="0-优化目标"><a href="#0-优化目标" class="headerlink" title="0. 优化目标"></a>0. 优化目标</h2><p>GPTQ 的本质是一个数学优化问题：如何修改权重，能使得量化前后的输出<strong>误差最小</strong>。这是为了解决一般的 PTQ 在权重四舍五入时精度损失问题。</p>
<p>目标：$$ \text{argmin}_{\hat{W}} || WX - \hat{W}X ||_2^2 $$</p>
<h2 id="1-前置知识：Optimal-Brain-Damage-OBD"><a href="#1-前置知识：Optimal-Brain-Damage-OBD" class="headerlink" title="1. 前置知识：Optimal Brain Damage (OBD)"></a>1. 前置知识：Optimal Brain Damage (OBD)</h2><p>OBD：移除网络中不重要的权重，可以期望获得更好的泛化能力、减少所需的训练示例数量，以及提高学习或分类速度。</p>
<p>OBD 认为：<strong>权重小不代表不重要。如果误差函数在这个权重方向上非常陡峭（二阶导数大），哪怕权重很小，删掉它也会导致 Loss 暴涨</strong>。</p>
<p>实际上上述内容就是权重剪枝，希望知道哪些参数删掉后对权重影响最少。而衡量重要性就要找到一个标准，于是提出**”权衡一个权重的重要性应该看看，删掉它后，Loss 的增加”**。</p>
<p>于是有了下面函数，给权重一个微小的导数，简单理解就是多元函数的泰勒展开取到二阶展开，此时 $H$ 就是海森公式：<br>$$<br>\delta E \approx \underbrace{\mathbf{g}^T \delta w}<em>{\text{一阶项}} + \underbrace{\frac{1}{2} \delta w^T \mathbf{H} \delta w}</em>{\text{二阶项}}<br>$$</p>
<ul>
<li>$\mathbf{g}^T$：梯度</li>
<li>$\mathbf{H}$：海森矩阵（Hessian Matrix）</li>
</ul>
<h4 id="1-三个假设："><a href="#1-三个假设：" class="headerlink" title="1. 三个假设："></a>1. 三个假设：</h4><p>海森矩阵并不好计算，于是有了三个假设：</p>
<ul>
<li>网络已经收敛了，意味着对参数的梯度为 0，那么一阶导为 0。</li>
<li>对角化，忽略了权重之间的影响。交叉项 $\frac{\partial^2 E}{\partial w_i \partial w_j} &#x3D; 0, i \neq j$，这样就进一步化简了二阶项，$\delta E \approx \frac{1}{2} \sum_k h_{kk} (\delta w_k)^2$。</li>
<li>假设 Loss 函数在局部就是一个碗状结构，意思是忽略更高阶导。</li>
</ul>
<h4 id="2-推导显著性公式（也就是删除某个权重后带来的影响）"><a href="#2-推导显著性公式（也就是删除某个权重后带来的影响）" class="headerlink" title="2. 推导显著性公式（也就是删除某个权重后带来的影响）"></a>2. 推导显著性公式（也就是删除某个权重后带来的影响）</h4><p>操作：删除 $w_k$，意味着让它的变化量 $\delta w_k &#x3D; -w_k$（也就是变成 0）。</p>
<p>损失变化：$$ \delta E_k \approx \frac{1}{2} h_{kk} (-w_k)^2 &#x3D; \frac{1}{2} h_{kk} w_k^2 $$（基于第二个假设）</p>
<p>这就意味着，我们不应该删除，权重本身很大，或者权重在这点二阶导很大的权重。</p>
<h4 id="3-分析"><a href="#3-分析" class="headerlink" title="3. 分析"></a>3. 分析</h4><p>实际上从三个假设来看，这种方式对后来有启发意义，删除的权重不能仅仅看权重大小，但是它忽略了权重之间的相关性（没有逆补偿机制）。</p>
<h2 id="2-前置知识：Optimal-Brain-Surgeon-OBS"><a href="#2-前置知识：Optimal-Brain-Surgeon-OBS" class="headerlink" title="2. 前置知识：Optimal Brain Surgeon (OBS)"></a>2. 前置知识：Optimal Brain Surgeon (OBS)</h2><p>OBS：也是最初用于<strong>剪枝</strong>，思考：让权重变为 0，同时让损失函数增加的最少?</p>
<p>OBS 相较于 OBD 有以下变动：（1）考虑二阶矩阵 $H$ 的全部（也就是相关性） （2）补偿机制，有人删，就要调整其他人来补偿。</p>
<h3 id="1-目标函数"><a href="#1-目标函数" class="headerlink" title="1. 目标函数"></a>1. 目标函数</h3><p>$$<br>\delta E \approx \mathbf{g}^T \delta w + \frac{1}{2} \delta w^T \mathbf{H} \delta w<br>$$</p>
<p>仍然假设网络收敛，去掉第一项。</p>
<h3 id="2-拉格朗日乘法求解"><a href="#2-拉格朗日乘法求解" class="headerlink" title="2. 拉格朗日乘法求解"></a>2. 拉格朗日乘法求解</h3><p>因为带有限制 $\mathbf{e}<em>q^T \delta \mathbf{w} &#x3D; -w_q$ ($\delta \mathbf{w}$ 的第 $q$ 位变化为 $-w_q$):<br>$$<br>\min</em>{\delta \mathbf{w}} \frac{1}{2} \delta \mathbf{w}^T \mathbf{H} \delta \mathbf{w} \quad \text{s.t.} \quad \mathbf{e}_q^T \delta \mathbf{w} &#x3D; -w_q<br>$$<br>构造函数：<br>$$<br>L &#x3D; \frac{1}{2} \delta \mathbf{w}^T \mathbf{H} \delta \mathbf{w} - \lambda (\mathbf{e}_q^T \delta \mathbf{w} + w_q)<br>$$<br>对 $\delta \mathbf{w}$ 求导并令为 0：$\mathbf{e}_q$ 是标准的 one-hot vector，第 $q$ 个位置为 1<br>$$<br>\mathbf{H} \delta \mathbf{w} - \lambda \mathbf{e}_q &#x3D; 0 \implies \delta \mathbf{w} &#x3D; \lambda \mathbf{H}^{-1} \mathbf{e}_q<br>$$<br>这意味最优权重更新 $\delta \mathbf{w}$，与海森矩阵的逆矩阵 $\mathbf{H}^{-1}$ 的第 $q$ 列成正比。</p>
<p>利用约束条件 $\mathbf{e}_q^T \delta \mathbf{w} &#x3D; -w_q$ 求解 $\lambda$：<br>$$<br>\mathbf{e}_q^T (\lambda \mathbf{H}^{-1} \mathbf{e}_q) &#x3D; -w_q<br>$$</p>
<p>$$<br>\lambda [\mathbf{H}^{-1}]<em>{qq} &#x3D; -w_q \implies \lambda &#x3D; \frac{-w_q}{[\mathbf{H}^{-1}]</em>{qq}}<br>$$</p>
<h3 id="3-OBS-更新方向"><a href="#3-OBS-更新方向" class="headerlink" title="3. OBS 更新方向"></a>3. OBS 更新方向</h3><p><strong>公式 A：最优更新方向（怎么改其他权重？）</strong></p>
<p>根据上述求解，可以得出 $\delta \mathbf{w}$ 的值，它的含义是当你删除 $w_q$ 这个权重时，其他权重应该如何变化：<br>$$<br>\delta \mathbf{w} &#x3D; - \frac{w_q}{[\mathbf{H}^{-1}]<em>{qq}} \cdot \mathbf{H}^{-1}</em>{:,q}<br>$$<br><strong>公式 B：显著性得分（Saliency，删掉它的代价是多少？）</strong><br>将 $\delta w$ 代回 $\delta E$ 公式：<br>$$<br>L_q &#x3D; \frac{1}{2} \frac{w_q^2}{[\mathbf{H}^{-1}]_{qq}}<br>$$<br><em>含义：我们应该计算所有权重的 $L_q$，然后删除 $L_q$ 最小的那个权重，而不是删除绝对值最小的。</em></p>
<h2 id="3-前置知识：OBQ-最优脑量化"><a href="#3-前置知识：OBQ-最优脑量化" class="headerlink" title="3. 前置知识：OBQ 最优脑量化"></a>3. 前置知识：OBQ 最优脑量化</h2><p><strong>OBQ</strong> 是 2022 年（GPTQ 的前身）提出的。它将 OBS 的思想从”剪枝”移植到了”量化”。</p>
<p><strong>核心区别</strong>：</p>
<ul>
<li><strong>OBS (剪枝)</strong>：约束是 $\delta w_q &#x3D; -w_q$（权重变 0）。</li>
<li><strong>OBQ (量化)</strong>：约束是 $\delta w_q &#x3D; \text{quant}(w_q) - w_q$（权重变最近的整数&#x2F;量化值）。设这个量化误差为 $\Delta_q$。</li>
<li>OBQ 包括 GPTQ 的目标函数变为了 $$ E_l &#x3D; || W_l X_l - \hat{W}_l X_l ||_2^2 $$（它们假设：<strong>如果我能让每一层的输出，在量化前后尽可能保持不变，那么整个网络的最终输出也就保持不变。</strong>）</li>
</ul>
<h4 id="1-OBQ-的数学更新"><a href="#1-OBQ-的数学更新" class="headerlink" title="1. OBQ 的数学更新"></a>1. OBQ 的数学更新</h4><p>直接套用 OBS 的推导（<strong>因为在 OBS 中的推导没有定义函数形式，所以等价</strong>），只需把约束值 $-w_q$ 换成 $\Delta_q$。</p>
<p><strong>OBQ 更新公式</strong>：<br>$$<br>\delta w &#x3D; \frac{\Delta_q}{[\mathbf{H}^{-1}]<em>{qq}} \cdot \mathbf{H}^{-1}</em>{:,q}<br>$$<br>这个公式告诉我们：<strong>如果你把第 $q$ 个参数强行量化了（产生了误差 $\Delta_q$），你应该如何调整剩下的未量化参数来补偿这个误差。</strong></p>
<h4 id="2-OBQ-的算法流程（贪心策略）"><a href="#2-OBQ-的算法流程（贪心策略）" class="headerlink" title="2. OBQ 的算法流程（贪心策略）"></a>2. OBQ 的算法流程（贪心策略）</h4><p>OBQ 是一个逐个处理权重的过程。对于一个全连接层（权重矩阵 $W$）：</p>
<ol>
<li><p><strong>计算 Hessian</strong>：$H &#x3D; 2XX^T$。（加个 2 方便计算）</p>
<p>推导：<br>$$<br>E &#x3D; || \hat{W}X - WX ||_2^2 \<br>E &#x3D; || (\hat{W} - W)X ||_2^2 \<br>E &#x3D; || \Delta W X ||_2^2\<br>E &#x3D; \Delta W (X X^T) \Delta W^T<br>$$</p>
</li>
<li><p><strong>计算逆矩阵</strong>：$H^{-1}$。</p>
</li>
<li><p><strong>循环迭代</strong>（直到所有权重都被量化）：</p>
<ul>
<li><strong>寻找最优 victim</strong>：遍历所有尚未量化的权重，计算如果量化它，会带来的误差代价（利用 OBS 的显著性公式变体）。<br>$$ \text{Error}<em>i &#x3D; \frac{(\text{quant}(w_i) - w_i)^2}{[H^{-1}]</em>{ii}} $$</li>
<li><strong>选择</strong>：找到 Error 最小的那个权重 $w_{\text{best}}$。</li>
<li><strong>量化</strong>：将 $w_{\text{best}}$ 量化，算出误差 $\Delta &#x3D; q - w_{\text{best}}$。</li>
<li><strong>补偿（核心步骤）</strong>：利用 OBQ 更新公式，把 $\Delta$ 通过 $H^{-1}$ 的相关性，”抹平”到剩余所有未量化的权重上。<ul>
<li>注意：此时剩余权重的数值变了！这意味着原本可能量化误差很大的数，被补偿后可能变得容易量化了。</li>
</ul>
</li>
<li><strong>更新 Hessian</strong>：因为 $w_{\text{best}}$ 已经固定了，它不再是变量，我们需要从 Hessian 矩阵中移除这一行一列（高斯消元）想具体了解可以参考：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1929142220717537176">GPTQ大模型量化算法原理详解（一） - 知乎</a>。</li>
</ul>
</li>
</ol>
<h4 id="3-OBQ-的问题"><a href="#3-OBQ-的问题" class="headerlink" title="3. OBQ 的问题"></a>3. OBQ 的问题</h4><p>OBQ 的精度极高（因为它不仅补偿，还每次都贪心地找最好处理的权重先处理）。<br>但是它<strong>太慢了</strong>。</p>
<ul>
<li>每次处理一个权重，都要更新整个权重矩阵。</li>
<li>每次都要在所有剩余权重里搜索下一个。</li>
<li>复杂度是 $O(d_{\text{row}} \cdot d_{\text{col}}^2)$。对于大模型（如 Llama 70B），这需要跑几百个小时，根本不可用。</li>
</ul>
<p><img src="/../images/OBS.jpg" alt="OBS的贪心策略（图片来自知乎）"></p>
<hr>
<h2 id="4-从-OBQ-到-GPTQ-的飞跃"><a href="#4-从-OBQ-到-GPTQ-的飞跃" class="headerlink" title="4. 从 OBQ 到 GPTQ 的飞跃"></a>4. 从 OBQ 到 GPTQ 的飞跃</h2><p>理解了 OBQ，你就明白 GPTQ 做了什么神操作：</p>
<ol>
<li><p><strong>放弃贪心搜索，固定顺序</strong>：<br>OBQ 说：”我要每次挑误差最小的先量化”。<br>GPTQ 说：”对于大模型，顺序其实不重要。我们就**按列顺序（1, 2, 3…）**量化，效果差不多。”(从上图可以看每行量化的权重是不同的顺序)</p>
<blockquote>
<p><strong>补充</strong>：虽然 GPTQ 放弃了贪心，但在实际工程中，如果能<strong>根据对角线元素 $H_{ii}$ 降序排列</strong>（先处理”硬骨头”），效果通常会更好。不过为了方便，直接按内存顺序处理也是主流做法。</p>
</blockquote>
<ul>
<li><strong>推导</strong>：这一步直接省去了每次 $O(d)$ 的搜索时间，并且让我们可以利用 Cholesky 分解预先算好 $H^{-1}$ 的信息，而不需要动态更新 $H^{-1}$。</li>
</ul>
<p><img src="/../images/GPTQ.jpg"></p>
</li>
<li><p><strong>Cholesky 分解：</strong></p>
<p>首先明白为什么要进行分解：</p>
<blockquote>
<p><strong>工程细节（阻尼 Damping）</strong>：在实际计算中，海森矩阵 $H$ 可能是奇异的（不可逆）。为了数值稳定性，我们通常会在对角线上加一个微小的正数 $\lambda$（如平均对角线值的 1%），即 $H’ &#x3D; H + \lambda I$，确保它是正定的，这样才能进行 Cholesky 分解。</p>
</blockquote>
<p>从过程入手：</p>
<p>量化补偿的计算如下：$$\delta w &#x3D; \frac{\Delta_q}{[\mathbf{H}^{-1}]<em>{qq}} \cdot \mathbf{H}^{-1}</em>{:,q}$$</p>
<p>需要 $H$ 矩阵的 $[q,q]$ 位置的值，以及 $H$ 矩阵的第 $q$ 列，那么在更新完后，第 $q$ 列就要删除了（否则后面的补偿使得前面的量化失败了），那么 $H$ 矩阵就要删除第 $q$ 行和第 $q$ 列。在 OBQ 中采取的方式是<strong>高斯消元法</strong>，有如下公式：（时间复杂度 ：$O(d^2)$ 如果有 $N$ 个参数则时间复杂度为 $O(Nd^2)$）<br>$$<br>H^{-1}<em>{\text{new}} &#x3D; H^{-1}</em>{\text{sub}} - \frac{\mathbf{v}\mathbf{v}^T}{h_{11}}<br>$$</p>
<p>我们从递归的视角看一下 <strong>Cholesky 分解</strong>：$A &#x3D; LL^T$ 目标就是给出一个矩阵将他分解为一个下三角矩阵。</p>
<blockquote>
<p><strong>注意</strong>：标准代码实现是对 $H$ 进行分解，但为了方便理解递归更新的原理，这里我们在数学推导上直接用 $H^{-1}$ 来演示，逻辑是等价的。</p>
</blockquote>
<p>为了方便理解，在推导过程直接用 $H^{-1}$ 来推导：<br>$$<br>H^{-1} &#x3D; \begin{bmatrix} a_{11} &amp; \mathbf{v}^T \ \mathbf{v} &amp; H^{-1}_{\text{sub}} \end{bmatrix}<br>$$</p>
<ul>
<li>$a_{11}$：左上角的元素。</li>
<li>$\mathbf{v}$：第一列剩下的部分。</li>
<li>$H^{-1}_{\text{sub}}$：右下角的子矩阵。</li>
</ul>
<p>同样，我们将目标矩阵 $L$ 也切分：<br>$$<br>L &#x3D; \begin{bmatrix} l_{11} &amp; 0 \ \mathbf{l} &amp; L_{\text{next}} \end{bmatrix}<br>$$</p>
<ul>
<li>$l_{11}$：左上角元素。</li>
<li>$\mathbf{l}$：第 1 列剩下的部分。</li>
<li>$L_{\text{next}}$：右下角的子下三角矩阵（我们要递归求的部分）。</li>
</ul>
<p>建立等式：$A &#x3D; L L^T$<br>$$<br>L L^T &#x3D; \begin{bmatrix} l_{11} &amp; 0 \ \mathbf{l} &amp; L_{\text{next}} \end{bmatrix} \times \begin{bmatrix} l_{11} &amp; \mathbf{l}^T \ 0 &amp; L_{\text{next}}^T \end{bmatrix} &#x3D; \begin{bmatrix} a_{11} &amp; \mathbf{v}^T \ \mathbf{v} &amp; H^{-1}_{\text{sub}} \end{bmatrix}<br>$$</p>
<p>（1）$$ a_{11} &#x3D; l_{11}^2 \implies l_{11} &#x3D; \sqrt{a_{11}} $$</p>
<p>（2）$$ \mathbf{v} &#x3D; l_{11}\mathbf{l} \implies \mathbf{l} &#x3D; \frac{1}{l_{11}}\mathbf{v} &#x3D; \frac{\mathbf{v}}{\sqrt{a_{11}}} $$</p>
<p>（3）$$ \mathbf{H^{-1}<em>{\text{sub}}} &#x3D; \mathbf{l}\mathbf{l}^T + L</em>{\text{next}}L_{\text{next}}^T $$ <strong>最关键的一步</strong></p>
<p>我们要解出 $L_{\text{next}}$（为了继续递归），需要把 $\mathbf{l}\mathbf{l}^T$ 移项到左边：<br>$$<br>\underbrace{L_{\text{next}}L_{\text{next}}^T}<em>{\text{下一步的目标}} &#x3D; \underbrace{\mathbf{H^{-1}</em>{\text{sub}}} - \mathbf{l}\mathbf{l}^T}<em>{\text{这就是 } \mathbf{H^{-1}</em>{\text{new}}}}<br>$$<br>然后带入 $\mathbf{l}$:<br>$$<br>\underbrace{H^{-1}<em>{\text{new}}}</em>{\text{更新后的逆矩阵}}<br>&#x3D; \underbrace{H^{-1}<em>{\text{sub}}}</em>{\text{右下角子矩阵}} - \underbrace{\mathbf{l}\mathbf{l}^T}<em>{\text{Cholesky 减去的项}}<br>&#x3D; \underbrace{H^{-1}</em>{\text{sub}} - \frac{\mathbf{v}\mathbf{v}^T}{h_{11}}}_{\text{OBQ&#x2F;高斯消元 减去的项}}<br>$$<br>如此迭代下去，所有的 $H$ 就提前算好了。</p>
</li>
<li><p><strong>Lazy Batch（懒惰批量）策略</strong></p>
<p>它把矩阵切成一个个 <strong>Block（比如 128 列一组）</strong>。</p>
<p><strong>步骤演示：</strong><br>假设我们处理第 1 个 Block（第 1~128 列）：</p>
<ol>
<li><p><strong>Block 内循环（在快速缓存中进行）：</strong></p>
<blockquote>
<p><strong>形象理解</strong>：这部分更新像一个<strong>三角形</strong>波浪。第 2 列依赖第 1 列的误差，第 3 列依赖前两列… 必须串行完成。</p>
</blockquote>
<ul>
<li>量化第 1 列 $\to$ 算出误差。</li>
<li><strong>只更新</strong> Block 内部剩下的列（第 2<del>128 列）。<strong>不碰</strong> Block 外面的第 129</del>10000 列。</li>
<li>量化第 2 列 $\to$ 算出误差。</li>
<li><strong>只更新</strong> Block 内部剩下的列（第 3~128 列）。</li>
<li>…</li>
<li>直到第 128 列量化完。</li>
</ul>
</li>
<li><p><strong>全局更新（GEMM 大招）：</strong></p>
<ul>
<li>现在，我们手里攒了 128 列的量化误差。</li>
<li>我们一次性把这 128 列的误差，更新到后面所有的列（129~End）。</li>
<li>这是一个 <strong>矩阵 $\times$ 矩阵</strong> 的乘法（GEMM）。</li>
<li><strong>$W_{\text{rest}} \leftarrow W_{\text{rest}} - \text{Block_Coeffs} \times \text{Block_Errors}$</strong></li>
</ul>
</li>
</ol>
<p><strong>为什么快？</strong></p>
<ul>
<li><strong>GEMV (矩阵-向量)</strong> $\to$ <strong>GEMM (矩阵-矩阵)</strong>。</li>
<li>GPU 对 GEMM 有极致优化（Tensor Core），数据读一次，可以进行很多次计算。</li>
<li><strong>内存带宽利用率</strong>从极低变成了极高。</li>
</ul>
</li>
</ol>
<h3 id="总结对照表"><a href="#总结对照表" class="headerlink" title="总结对照表"></a>总结对照表</h3><table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left">OBS (1993)</th>
<th align="left">OBQ (2022)</th>
<th align="left">GPTQ (2023)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>目标</strong></td>
<td align="left">剪枝 (Pruning)</td>
<td align="left">量化 (Quantization)</td>
<td align="left">量化 (Quantization)</td>
</tr>
<tr>
<td align="left"><strong>操作</strong></td>
<td align="left">权重设为 0</td>
<td align="left">权重设为 Int</td>
<td align="left">权重设为 Int</td>
</tr>
<tr>
<td align="left"><strong>策略</strong></td>
<td align="left">全局寻找最不重要的权重</td>
<td align="left">贪心寻找误差最小的权重</td>
<td align="left"><strong>固定顺序 (按列)</strong></td>
</tr>
<tr>
<td align="left"><strong>核心原理</strong></td>
<td align="left">二阶泰勒展开 + 拉格朗日乘子</td>
<td align="left">同 OBS</td>
<td align="left">同 OBS</td>
</tr>
<tr>
<td align="left"><strong>计算量</strong></td>
<td align="left">极大</td>
<td align="left">大 (大模型跑不动)</td>
<td align="left"><strong>极快 (分块优化)</strong></td>
</tr>
<tr>
<td align="left"><strong>补偿机制</strong></td>
<td align="left">删除一个，调整其余</td>
<td align="left">量化一个，调整其余</td>
<td align="left">量化一列，调整后续列</td>
</tr>
</tbody></table>
<script src="https://giscus.app/client.js"
        data-repo="Alielie-9527/Alielie.github.io"
        data-repo-id="R_kgDOQvfoCQ"
        data-category="General"
        data-category-id="DIC_kwDOQvfoCc4C0SEV"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="preferred_color_scheme"
        data-lang="zh-CN"
        crossorigin="anonymous"
        async>
</script>
    </div>
    
    
    
    
    <div id="comment">
        <div id="giscus-container" class="giscus"></div>
    </div>
    
    
    
    
</div>

            <footer id="footer">
    <div id="footer-wrap">
        <div>
            &copy;
            2025 - 2025 Alielie&#39;s Blog
            <span id="footer-icon">
                <i class="fa-solid fa-font-awesome fa-fw"></i>
            </span>
            &commat;Alielie
        </div>
        <div>
            Based on the <a target="_blank" rel="noopener" href="https://hexo.io">Hexo Engine</a> &amp;
            <a target="_blank" rel="noopener" href="https://github.com/theme-particlex/hexo-theme-particlex">ParticleX Theme</a>
        </div>
        
    </div>
</footer>

        </div>
        
        <transition name="fade">
            <div id="preview" ref="preview" v-show="previewShow">
                <img id="preview-content" ref="previewContent" />
            </div>
        </transition>
        
    </div>
    <script src="/js/main.js"></script>
    
    
<script
    src="https://giscus.app/client.js"
    data-repo="Alielie-9527/alielie-9527.github.io"
    data-repo-id="R_kgDOQvfoCQ"
    data-category="General"
    data-category-id="DIC_kwDOQvfoCc4C0SEV"
    data-mapping="pathname"
    data-strict="0"
    data-reactions-enabled="1"
    data-emit-metadata="0"
    data-input-position="bottom"
    data-theme="preferred_color_scheme"
    data-lang="zh-CN"
    crossorigin
    async
></script>





    
</body>
</html>
